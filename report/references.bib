
@book{james_introduction_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2020-03-21},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	doi = {10.1007/978-1-4614-7138-7},
	file = {James et al. - 2013 - An Introduction to Statistical Learning.pdf:C\:\\Users\\jritt\\Zotero\\storage\\NWCPQN3V\\James et al. - 2013 - An Introduction to Statistical Learning.pdf:application/pdf}
}

@book{hastie_elements_2009,
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84858-7},
	url = {https://books.google.co.uk/books?id=tVIjmNS3Ob8C},
	publisher = {Springer New York},
	author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	year = {2009},
	lccn = {2008941148},
	file = {(Springer Series in Statistics) Trevor Hastie, Robert Tibshirani, Jerome Friedman - The elements of statistical learning_ Data mining, inference, and prediction-Springer (2009).pdf:C\:\\Users\\jritt\\Zotero\\storage\\ZRW2JEJR\\(Springer Series in Statistics) Trevor Hastie, Robert Tibshirani, Jerome Friedman - The elements of statistical learning_ Data mining, inference, and prediction-Springer (2009).pdf:application/pdf}
}

@book{noauthor_computer_nodate,
	title = {Computer {Age} {Statistical} {Inference}},
	language = {en},
	file = {(Institute of Mathematical Statistics Monographs) Bradley Efron, Trevor Hastie - Computer Age Statistical Inference_ Algorithms, Evidence, and Data Science-Cambridge University Press (2016).pdf:C\:\\Users\\jritt\\Zotero\\storage\\4NFXEI73\\(Institute of Mathematical Statistics Monographs) Bradley Efron, Trevor Hastie - Computer Age Statistical Inference_ Algorithms, Evidence, and Data Science-Cambridge University Press (2016).pdf:application/pdf}
}

@book{johnson_applied_2014,
	address = {Harlow},
	edition = {6. ed., new internat. ed},
	series = {Always learning},
	title = {Applied multivariate statistical analysis},
	isbn = {978-1-292-02494-3},
	language = {en},
	publisher = {Pearson Education Limited},
	author = {Johnson, Richard Arnold and Wichern, Dean W.},
	year = {2014},
	note = {OCLC: 881298386},
	file = {Johnson and Wichern - 2014 - Applied multivariate statistical analysis.pdf:C\:\\Users\\jritt\\Zotero\\storage\\FSQBP36F\\Johnson and Wichern - 2014 - Applied multivariate statistical analysis.pdf:application/pdf}
}

@article{griggs_penalized_nodate,
	title = {Penalized {Spline} {Regression} and its {Applications}},
	language = {en},
	author = {Griggs, Whitney},
	pages = {51},
	file = {Griggs - Penalized Spline Regression and its Applications.pdf:C\:\\Users\\jritt\\Zotero\\storage\\69YJS4CK\\Griggs - Penalized Spline Regression and its Applications.pdf:application/pdf}
}

@article{sagan_there_nodate,
	title = {“{There} are naive questions, tedious questions, ill-phrased questions, questions put after inadequate self-criticism. {But} every question is a cry to understand the world. {There} is no such thing as a dumb question”.},
	language = {en},
	author = {Sagan, Carl},
	pages = {2},
	file = {Sagan - “There are naive questions, tedious questions, ill.pdf:C\:\\Users\\jritt\\Zotero\\storage\\PDZM8WXB\\Sagan - “There are naive questions, tedious questions, ill.pdf:application/pdf}
}

@book{agresti_categorical_2002,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Categorical data analysis},
	isbn = {978-0-471-36093-3},
	language = {en},
	publisher = {Wiley-Interscience},
	author = {Agresti, Alan},
	year = {2002},
	keywords = {Multivariate analysis},
	file = {Agresti - 2002 - Categorical data analysis.pdf:C\:\\Users\\jritt\\Zotero\\storage\\V72S64TL\\Agresti - 2002 - Categorical data analysis.pdf:application/pdf}
}

@article{zhu_two-component_2016,
	title = {A two-component mixture model for density estimation and classification},
	volume = {19},
	issn = {0972-0502, 2169-012X},
	url = {http://www.tandfonline.com/doi/full/10.1080/09720502.2015.1103477},
	doi = {10.1080/09720502.2015.1103477},
	abstract = {Density estimation is one of the most important techniques that have been used to solve machine-learning problems. The mixture model is a useful tool for density estimation and classification. In this paper, we describe a simple two-component mixture model for density estimation. The associated EM algorithm for carrying out maximum likelihood estimation is also proposed. An application of the proposed model and approach to discriminant the well-known data from the Wisconsin breast cancer study is demonstrated. The experimental results show that the proposed model provides a good way to the classification problems.},
	language = {en},
	number = {2},
	urldate = {2020-09-24},
	journal = {Journal of Interdisciplinary Mathematics},
	author = {Zhu, Degang},
	month = mar,
	year = {2016},
	pages = {311--319},
	file = {Zhu - 2016 - A two-component mixture model for density estimati.pdf:C\:\\Users\\jritt\\Zotero\\storage\\LZHZPTPX\\Zhu - 2016 - A two-component mixture model for density estimati.pdf:application/pdf}
}

@article{grosse_lecture_nodate,
	title = {Lecture 16: {Mixture} models},
	language = {en},
	author = {Grosse, Roger and Srivastava, Nitish},
	pages = {16},
	file = {Grosse and Srivastava - Lecture 16 Mixture models.pdf:C\:\\Users\\jritt\\Zotero\\storage\\UT8RCLMJ\\Grosse and Srivastava - Lecture 16 Mixture models.pdf:application/pdf}
}

@article{egbo_comparison_2014,
	title = {A {Comparison} of the {Optimal} {Classification} {Rule} and {Maximum} {Likelihood} {Rule} for {Binary} {Variables}},
	volume = {6},
	issn = {1916-9809, 1916-9795},
	url = {http://www.ccsenet.org/journal/index.php/jmr/article/view/42115},
	doi = {10.5539/jmr.v6n4p124},
	abstract = {Optimal classification rule and maximum likelihood rules have the largest possible posterior probability of correct allocation with respect to the prior. They have a ‘nice’ optimal property and appropriate for the development of linear classification models. In this paper we consider the problem of choosing between the two methods and set some guidelines for proper choice. The comparison between the methods is based on several measures of predictive accuracy. The performance of the methods is studied by simulations.},
	language = {en},
	number = {4},
	urldate = {2020-09-28},
	journal = {Journal of Mathematics Research},
	author = {Egbo, I. and Onyeagu, S. I. and Ekezie, D. D. and Peter O., Uzoma},
	month = nov,
	year = {2014},
	pages = {p124},
	file = {Egbo et al. - 2014 - A Comparison of the Optimal Classification Rule an.pdf:C\:\\Users\\jritt\\Zotero\\storage\\JV5P9XF5\\Egbo et al. - 2014 - A Comparison of the Optimal Classification Rule an.pdf:application/pdf}
}

@article{eriksson_lecture_nodate,
	title = {Lecture 8: {Classification}},
	language = {en},
	author = {Eriksson, Måns},
	pages = {26},
	file = {Eriksson - Lecture 8 Classification.pdf:C\:\\Users\\jritt\\Zotero\\storage\\JYFBB49N\\Eriksson - Lecture 8 Classification.pdf:application/pdf}
}

@article{helwig_discrimination_nodate,
	title = {Discrimination and {Classification}},
	language = {en},
	author = {Helwig, Nathaniel E},
	pages = {74},
	file = {Helwig - Discrimination and Classification.pdf:C\:\\Users\\jritt\\Zotero\\storage\\EZ248S8P\\Helwig - Discrimination and Classification.pdf:application/pdf}
}

@book{schapire_boosting_2012,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning series},
	title = {Boosting: foundations and algorithms},
	isbn = {978-0-262-01718-3},
	shorttitle = {Boosting},
	language = {en},
	publisher = {MIT Press},
	author = {Schapire, Robert E. and Freund, Yoav},
	year = {2012},
	keywords = {Boosting (Algorithms), Supervised learning (Machine learning)},
	file = {Schapire and Freund - 2012 - Boosting foundations and algorithms.pdf:C\:\\Users\\jritt\\Zotero\\storage\\6E4JTP5V\\Schapire and Freund - 2012 - Boosting foundations and algorithms.pdf:application/pdf}
}

@article{goel_non-discriminatory_2018,
	title = {Non-{Discriminatory} {Machine} {Learning} through {Convex} {Fairness} {Criteria}},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278722},
	doi = {10.1145/3278721.3278722},
	abstract = {Biased decision making by machine learning systems is increasingly recognized as an important issue. Recently, techniques have been proposed to learn non-discriminatory classiﬁers by enforcing constraints in the training phase. Such constraints are either non-convex in nature (posing computational difﬁculties) or don’t have a clear probabilistic interpretation. Moreover, the techniques offer little understanding of the more subjective notion of fairness.},
	language = {en},
	urldate = {2020-10-16},
	journal = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
	month = dec,
	year = {2018},
	pages = {116--116},
	file = {Goel et al. - 2018 - Non-Discriminatory Machine Learning through Convex.pdf:C\:\\Users\\jritt\\Zotero\\storage\\8NZLQ8V2\\Goel et al. - 2018 - Non-Discriminatory Machine Learning through Convex.pdf:application/pdf}
}

@inproceedings{buolamwini_gender_2018,
	address = {New York, NY, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	volume = {81},
	url = {http://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	editor = {Friedler, Sorelle A. and Wilson, Christo},
	month = feb,
	year = {2018},
	pages = {77--91}
}

@article{datta_automated_2015,
	title = {Automated {Experiments} on {Ad} {Privacy} {Settings}},
	volume = {2015},
	url = {https://content.sciendo.com/view/journals/popets/2015/1/article-p92.xml},
	doi = {https://doi.org/10.1515/popets-2015-0007},
	number = {1},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Datta, Amit and Tschantz, Michael Carl and Datta, Anupam},
	month = apr,
	year = {2015},
	note = {Place: Berlin
Publisher: Sciendo},
	pages = {92 -- 112}
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	url = {http://science.sciencemag.org/content/366/6464/447.abstract},
	doi = {10.1126/science.aax2342},
	abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	number = {6464},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pages = {447}
}

@book{james_islr_2013,
Author = {James, Gareth and Tibshirani, Robert and SpringerLink (Online, service) and Hastie, Trevor and Witten, Daniela},
ISBN = {9781461471370},
Publisher = {Springer New York},
Series = {Springer Texts in Statistics: 103},
Title = {An Introduction to Statistical Learning. [Elektronisk resurs] with Applications in R.},
URL = {http://ludwig.lub.lu.se/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=cat07147a&AN=lub.5846582&site=eds-live&scope=site},
Year = {2013},
}

@article{calders_verwer_2010,
author = {Calders, Toon and Verwer, Sicco},
year = {2010},
month = {09},
pages = {277-292},
title = {Three naive Bayes approaches for discrimination-free classification},
volume = {21},
journal = {Data Min. Knowl. Discov.},
doi = {10.1007/s10618-010-0190-x}
}

@InProceedings{zemel_lfr_2013, 
title = {Learning Fair Representations}, 
author = {Rich Zemel and Yu Wu and Kevin Swersky and Toni Pitassi and Cynthia Dwork}, pages = {325--333}, year = {2013}, editor = {Sanjoy Dasgupta and David McAllester}, volume = {28}, number = {3}, series = {Proceedings of Machine Learning Research}, address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v28/zemel13.pdf}, url = {http://proceedings.mlr.press/v28/zemel13.html}, abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.} }

@unknown{feng_adversial_2019,
author = {Feng, Rui and Yang, Yang and Lyu, Yuehan and Tan, Chenhao and Sun, Yizhou and Wang, Chunping},
year = {2019},
month = {04},
pages = {},
title = {Learning Fair Representations via an Adversarial Framework}
}

@article{creager_learningrep_2019,
Abstract = {We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.},
Author = {Creager, Elliot and Madras, David and Jacobsen, Jörn-Henrik and Weis, Marissa A. and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard},
Keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
Title = {Flexibly Fair Representation Learning by Disentanglement.},
URL = {http://ludwig.lub.lu.se/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=edsarx&AN=edsarx.1906.02589&site=eds-live&scope=site},
Year = {2019},
}

@article{kamiran_calders_preprocessing_2011,
author = {Kamiran, Faisal and Calders, Toon},
year = {2011},
month = {10},
pages = {},
title = {Data Pre-Processing Techniques for Classification without Discrimination},
volume = {33},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-011-0463-8}
}

@article{hardt_equality_2018,
  author    = {Moritz Hardt and
               Eric Price and
               Nathan Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1610.02413},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02413},
  archivePrefix = {arXiv},
  eprint    = {1610.02413},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HardtPS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
