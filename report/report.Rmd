---
title: 'Investigating discrimination bias in predictive modelling'
author:
  - Jonathan Rittmo
  - Sara Thiringer

date: "`r Sys.Date()`"
bibliography: references.bib
toc: false
link-citations: yes
linkcolor: blue
numbersections: false
indent: true
csl: apa.csl
output: 
  bookdown::pdf_document2: default
header-includes: |
  \usepackage{setspace}\onehalfspacing
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fairmodels)
library(tidyverse)
library(knitr)
library(kableExtra)
library(summarytools)
library(tidymodels)

compas <- fairmodels::compas
COMPAS <- filter(compas, Ethnicity == "African_American" | Ethnicity == "Caucasian")
#compas$Two_yr_Recidivism <- as.factor(ifelse(compas$Two_yr_Recidivism == '1', '0', '1'))
COMPAS$Ethnicity <- droplevels(COMPAS$Ethnicity)

split <- initial_split(COMPAS, prop = 0.8, strata = "Two_yr_Recidivism")
compas_train <- training(split)
y_numeric <- as.numeric(compas_train$Two_yr_Recidivism)-1
```

# Introduction

As more data and consequently more data-driven decisions have entered the world,
the problem with algorithms reinforcing discriminatory structures have received
an increasing amount of attention. Several cases showcase how algorithms that
were designed to be neutral decision-makers have made discriminatory
predictions. Examples include facial recognition being less accurate for people
with darker skin [@buolamwini_gender_2018], ads for higher paid jobs being shown
more frequently to men [@datta_automated_2015], healthcare predictions
underestimating the illness of black people [@obermeyer_dissecting_2019] as well
as individual tech company scandals such as [the Apple card seemingly granting
men a higher credit limit than
women](https://www.washingtonpost.com/business/2019/11/11/apple-card-algorithm-sparks-gender-bias-allegations-against-goldman-sachs/)
and [Amazon’s automated recruitment tool unrighteously favoring
men](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
However unintentional, these examples show the need for an awareness of
discrimination and fairness when collecting data, training models and using
predictions for decision-making.

In this project we have looked at methods for dealing with potential
discrimination in models. First, we present x ways of measuring fairness in
order to be able to evaluate it. In this part we also discuss different methods
of dealing with bias. Secondly, we train several statistical models on a dataset
were we know that discriminatory bias is present. We then use the R package
*fairmodels* to evaluate how the model performed both in terms of accuracy and
fairness. Lastly, we use a pre-processing bias mitigation method called impact
disparate remover to reduce the presence of bias in the models. We evaluate the
success of this tool and discuss it in relation to the different models.

For simplicity, this project focuses solely on classification methods. (maybe present more restrictions)   

[Something on terminology... Maybe small appendix.]  

## Understanding Fairness in Statistical Models

In order to build models that are less discriminatory than previously mentioned
examples have proven to be, we first need a way to measure fairness. In recent
years, a lot of work have been put into defining fairness in a quantitative way
for machine learning purposes. These different definitions, listed and explained
below, implicate somewhat different views on what fairness really is. This
ongoing discussion draws on previous social and legal research on equality and
fairness. There are several measures available for such evaluation, but for 
simplicity we will focus on two of the most common.

### Demographic parity

This criterion states that the target variable should be independent of
"protected" characteristics (such as race and gender). This means that if $Y$ is
our decision and $X$ the protected variable in question:
$$
P(Y=1 | X=1) = P(Y=1 | X=0)
$$
This rule will enhance rules which classify the same fraction in each group as
positive. This might appear unappealing, as it has a way of "forcing" equality.
However, as @goel_non-discriminatory_2018 point out, many classifiers impact
decisions which create future labels in future training data, and as such a lack
of demographic parity risks causing ever-looping cycles of discrimination.
Another negative with this measure, however, is that it makes sure we cannot use
$X$ as a predictor which we in some circumstances want.

### Equalised odds

This criterion proposes that the predictor we use and the
protected variable should be independent conditional on the true outcome, i.e.
if $G$ is the predictor we want to use:
$$
P(G=1|X=0, Y=1) = P(G=1|X=1, Y=1)
$$
This measure ensures that the true positive rates of the classifier are the same for both groups of $X$.

## Optimising for Fairness

As we already know, statistical learning may have other goals than just
maximizing accuracy. For example, different costs for different types of
misclassification may lead us to choose a different model than the one that has
the highest overall accuracy rate [@james_islr_2013]. Tuning for a higher rate
of fairness introduces yet another way to both pre-process data, train models
and make predictions. Since discriminatory bias can be introduced in several
different ways and at different stages of model training, there are several
methods to optimize for fairness.

### Pre-Processing

Since discrimination may have been present in both data collection and the
systems (institutions, products, etc) that generated data, we might want to deal
with the issue already in the pre-processing stage. Attempts to do this includes
resampling and synthesizing data. For example, one approach with several
suggested algorithms proposes mapping the datapoints to a new feature space were
as much relevant information as possible is kept while information about the
protected variable is lost [@creager_learningrep_2019; @feng_learning_2019; @zemel_lfr_2013].
Resampling, a technique commonly used to deal with
undersampled groups or otherwise skewed data, have also proven to be an
alternative for moderating discrimination in statistical learning.
@kamiran_data_2012 suggests reweighing and resampling
observations so as to enhance the presence of an initially undersampled groups
(for example high income women). Here we will deal with three preprocessing 
techniques:

* Disparate impact removal 
    - Introduced in @feldman_certifying_2015. This is a geometric method reshaping 
      distributions on numerical variables conditioned on the protected. 

* Two resampling methods introduced in @kamiran_data_2012. In a scenario
  with a binary protected variable and a binary classification problem
  the four sample sizes relating to the joint distribution of classes
  and protected features that would make the dataset discrimination-free
  are calculated. Stratified sampling from these two groups are then performed.
  Observations are then resampled with one out of two techniques. 
    - Uniform resampling, where each observation has equal porbability to 
      be duplicated or skipped to increase or decrease the size of a group.
    - Preferential resampling, where observations close to the decision boundary
      (of a classifier) gets higher priority for being duplicated or skipped.

### Training

Naturally, many methods for mitigating discrimination by algorithms focus on the
training phase of statistical learning. From avoiding overfitting, we know how
to constrain the learning by penalizing learned parameters and thus restraining
them from having an unproportionally large impact in a classifier. Among
classical apporaches ridge and lasso regression can be mentioned
[@hastie_elements_2009]. Bias have been shown to sometimes remain in models
despite the removal of the protected attribute, due to their correlation to
other non-sensitive attributes (källa). In this sense, penalizing variables that
are highly correlated to protected features looks like a promising solution.

A similar approach, also focusing on the learning phase, is the *weighted sum of
logs technique* propsed by [@goel_non-discriminatory_2018]. This algorithm
models both the historical bias in the data and emprical bias imposed by the
original algorithm in order to establish a "poportionally fair" classifier by
restricting the optimization space of a logistic regression model.

posing constraint in the learning phase, dealing with correlations, penalty

It has even been suggested that one solution could be to build a multiple of
models - i.e. one for each social group. Such an approach have proved to be
succesful in some cases [@calders_verwer_2010], but has a clear disatvantage
when it comes to dealing with multiple (intertwined) protected variables.

We will not deal with any such methods though, so perhaps remove this section?

#### Prediction

Finally, another way to remove discriminatory outcomes is by post-processing the
predictions. For example, we normally calculate the best possible cut-off value
with regards to accuracy in classificaiotn models (@hastie_elements_2009).
Instead of only optimising for accuracy, we could include a measure of fairness
and have it play a role in the decision of the threshold value. Another similar
technique is to modify the cut-off value differently for each group, as proposed
by @hardt_equality_2018, although they do point out that a better option is to
first "invest in better features and more data". A benefit to this approach is
that it is much less complex and hence has a more straight-forward
implementation than many of the previously mentioned techinques. We will look at
how changing cut-off values with regards to minimisng demographic parity and
equalised odds affect model accuracy.

# Analysis

## Classification on the COMPAS Data

The case we have selected to use for building statistical learning models,
evaluating their discrimintion bias and furthermore mitigating these effects, is
the compas^[Correctional Offender Management Profiling for Alternative
Sanctions] data made available by Florida Department of Corrections and
ProPublica in 2016. The dataset sourced from the `fairmodels` package, slightly
modified from the original COMPAS dataset, is a collection of $n = 6172$ observations with the
variables: ` `r names(compas)` `. Where `Two_yr_Recidivism` is the variable we
want to predict and indicates whether an inmate have made another offense within
two years of release. As such, the dataset is being used to decide on the
penalty of criminals. The compas data is an excellent case for studying
discriminatory algorithms, as it both exhibits bias and is being used for actual
classification for a highly sensitive cause. Because of this, many researchers
who explore fairness in machine learning have chosen to study it as an example.
For the present purpose we filtered the data on `Ethnicity`, for `Caucasian` and
`African_American` to create a binary protected
variable. Rendering $n$ to a total of $5278$ with no missing values. A summary
of the dataset can be seen below.
```{r desc, results='asis', echo=FALSE}

dfSummary(COMPAS, silent = TRUE, graph.col = FALSE, valid.col = FALSE)

```

### Preprocessed data

The two resampling methods aims to even out inequalities between the depraved
and privileged groups having positive and negative outcome attributes
respectively. That is, in our case, the methods try to make the joint
distribution of `Ethnicity` and `Two_yr_Recidivism` uniform by duplicating some
observations and removing others. In effect undersampling some groups and oversampling
others. If we look at the original joint distribution of `Ethnicity` and `Two_yr_Recidivism`
from the training data in table \@ref(tab:joint-org).

```{r joint-org, results='asis', echo=FALSE}

kable(table(compas_train$Two_yr_Recidivism, compas_train$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

# kable(joined, align = "r", digits = 2, booktabs = T, 
#       caption = "Joint distribution of Ethnicity and Two_yr_Recidivism", 
#       format = "latex",
#       col.names = c("Deficit",
#                     "n",
#                     "TD",
#                     "TD",
#                     "BTD"))%>%
#   kable_styling(latex_options = c("hold_position")) %>% 
#   collapse_rows(columns= 1, valign = "top", latex_hline = "major") %>% 
#   add_header_above(c(" " = 2, "Analytic" = 1,  "Simulation" = 2)) 

```

In uniform resampling all observations have equal probability of being
duplicated or removed. However, in preferential resampling we fit a logisitc
regression model with `Two_yr_Recidivism` as outcome variable and all the other
variables as predictors, then we use the resulting probabilities as sampling
probabilities, where borderline cases (i.e. cases that is on the border of being
classified as either 1 or 0 in `Two_yr_Recidivism`) are duplicated or removed
more frequently than "certain" cases. In table \@ref(tab:joint-uni) the new
joint distribution when using uniform resampling can be seen and in table
\@ref(tab:joint-pref) preferential sampling has been used.

```{r joint-uni, results='asis', echo=FALSE}
uniform_indexes <- resample(protected = compas_train$Ethnicity,
                            y = y_numeric)

kable(table(compas_train[uniform_indexes,]$Two_yr_Recidivism, compas_train[uniform_indexes,]$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism, uniform resampling",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

```

```{r joint-pref, results='asis', echo=FALSE}
probs <- glm(Two_yr_Recidivism ~., data = compas_train, family = binomial())$fitted.values
pref_ind <- resample(protected = compas_train$Ethnicity,
                     y = y_numeric,
                     type = "preferential",
                     probs = probs)

kable(table(compas_train[pref_ind,]$Two_yr_Recidivism, compas_train[pref_ind,]$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism, preferential resampling",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

```

As can be seen, the distributions are similar even though different observations have
been resampled. 

## Models

We trained five different models on the COMPAS data tuned for various
parameters. Each of these models was then fit again but on data having
undergone one of the three preprocessing mitigation methods. We fit the 
models using all predictors in the dataset including the protected variable.
The K-nearest neighbour model turned out to perform worse than chance when
fit on mitigated datasets and was therefore left out of the analysis. The
dataset was split for training and testing with 20% of the observations
reserved for testing.

Model  | Tuning | Abbreviation
------ | ------|-------------------------------------------
Random Forest | Number of predictors sampled at each split | RF
Artificial neural net  | Number of hidden nodes | ANN
Logistic ridge regression | Penalisation | LR
K-nearest neighbour (left out) | Number of neighbours | KNN
AdaBoost | Number of predictors sampled at each split | None 


### Model evaluation

When evaluating bias in machine learning models one often utilise the ratio of
the terms on either side of the equality in e.g. demographic parity or equalised
odds described previously. I.e. if
$$
\frac{ P(Y=1 | X=0)}{P(Y=1 | X=1)} =1 \ \ \text{and/or} \ \  \frac{P(G=1|X=0, Y=1)}{P(G=1|X=1, Y=1)} =1
$$
we can say that our model is completely fair. In 1979 the US Equal Employment
Opportunity Commission deemed that if this ratio falls below 0.8 disparate
impact is present, known as the 80% rule [@feldman_certifying_2015]. Hence, we
want a model that satisfies the above criteria as closely as possible but still
yield a high accuracy. Let us for simplicity solely consider demographic parity
(this also often referred to as statistical parity) as a measure for now. We can
then compare the acquired accuracy of each model together with parity loss
evaluated by this measure (note the acquired accuracy is measured on training
data) as seen in figure \@ref(fig:fap1).

(ref:fap1-text) Accuracy and fairness comparison for training data on the fitted models. Note that the y-axis is inverted. This is not the ratio explained previously but the "parity loss" which should be interpreted as the difference between the parity ratio and 1 [IS THIS TRUE??]. Hence, a measure that we want to minimise. 

```{r fap1, echo=FALSE, fig.cap="(ref:fap1-text)", out.width='100%', results='hide'}

load("models/fobject1.Rdata")
fap <- performance_and_fairness(fobject1, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x + 
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model")
```

Figure \@ref(fig:fap1) tells us that using demographic parity as a fairness measure
none of our models satisfy the 80% rule and thus disparate impact is present. Using
the preprocessing techniques previously described we now fit the models on these
processed datasets and compare which model and processing technique yields best
results both in terms of fairness and accuracy (upper right corner of \@ref(fig:fap1)).
The results can be seen in \@ref(fig:fap-all).

(ref:fap-all-text) Accuracy and fairness comparison for preprocessed training data on the fitted models. The three methods can be seen in the suffixes to the models: "rem" = disparate impact removal, "resa pref" = preferential resampling and "resa unif" = uniform resampling. Note that the y-axis is inverted.

```{r fap-all, echo=FALSE, fig.cap="(ref:fap-all-text)", out.width='100%', fig.height=6, results='hide'}

load("models/fobject_all.Rdata")
fap <- performance_and_fairness(fobject_all, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x + 
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model")
```

The resampling methods seems to have best effect on minimising parity loss
and the disparate impact removal technique even increase parity loss for
some models. This might be due to the fact that this method can only mitigate
numerical variables and there is only the `Number_of_Priors` variable in this dataset
that we can mitigate. However, most of the resampled models have a parity loss of less
than 0.2 (i.e. they satisfy the 80% rule). Choosing the four most accurate of these
we can investigate whether applying a different probability cutoff than the conventional
0.5 would mitigate parity loss even further. In figure \@ref(fig:cutoff-rf) it can be
seen how the parity measures are affected by varying the cutoffs and the values that
would minimise demographic parity *and* equalised odds. 

```{r cutoff-rf, echo=FALSE, fig.cap="CUTOFF PLOT", out.width='100%',  results='hide', message=FALSE}

load("models/fobject_co.Rdata")

plot(ceteris_paribus_cutoff(fobject_co,
                            subgroup = "African_American",
                            fairness_metrics = c("TPR","STP"))) +
  ggtitle("", subtitle = "") +
  labs(x = "Cutoff value", y = "Parity loss", color = "Parity loss metric") +
  scale_color_discrete(labels = c("Demographic parity", "Equalised odds")) 
  
```
Applying the cutoff values from figure \@ref(fig:cutoff-rf) and comparing them
to models using 0.5 we can evaluate whether this drop in parity loss would
be worth any potential drop in accuracy. This comparison can be seen in
figure \@ref(fig:cutoff-rf). Adjusting cutoffs can have quite large effect,
however, since all models already satisfies the 80% rule we want to argue
that the drop in accuracy is not worth it. Hence, if the goal of the analysis
is to find a single model we would choose Random Forest trained on uniformly
resampled data. As a coincidence it turns out that this model produces the 
smallet parity loss as measured by equlised odds, further increasing our confidence.


```{r fap-rf-comp, echo=FALSE, fig.cap="RF comparison", out.width='100%',  results='hide'}

load("models/fo_comp.Rdata")

fap <- performance_and_fairness(fo_comp, fairness_metric = "STP")
plot(fap) +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") 
  
```

## Final model evaluation

Since we now have a chosen model we want to see how it performs on out of sample
data. Due to the nature of our analysis it might also be interesting to see how
well it performs in comparison to the other models fitted. Accuracy and parity
loss as measured by demographic parity for all models using unseen testing data
can be seen in figure \@ref(fig:fap-test) and our chosen model appears to have a
satisfactory performance on test data as well, however, it just barely meets the
80% criterion and it is no longer the model with best accuracy among the ones 
with a demographic parity below $0.2$. 

```{r fap-test, echo=FALSE, fig.cap="Testing", out.width='100%', fig.height=6,  results='hide'}

load("models/fobject_test.Rdata")

fap <- performance_and_fairness(fobject_test, fairness_metric = "STP")
plot(fap) +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") 
  
```

# Discussion

We have in this report investigated methods to mitigate discriminatory bias in
machine learning. Specifically we have analysed the COMPAS dataset to predict
whether an offender will commit another crime within two years after release.
This is obviously useful information when judging whether to release inmates or
not. However, to do this we want models that are accurate but do not take
certain sensitive variables into account when making predictions. Several
measures are available to estimate fairness in models each with slightly
different interpretation. We have only addressed two of these measures in this
report and even further limited our analysis by mainly taking demographic parity
into account when evaluating models. Our results show that it is possible to
limit the effect of unfair models to a great extent without losing too much
accuracy. However, caution is advised for using this data to inform decisions as
sensitive as whether to release an inmate or not due to the accuracy (of any
model) not even exceeding 70%.


# References

