---
title: 'Project Proposal'
author:
  - Jonathan Rittmo
  - Sara Thiringer
date: "`r Sys.Date()`"
# bibliography: Dissertation.bib
toc: false
link-citations: yes
linkcolor: blue
numbersections: false
indent: true
csl: apa.csl
output: 
  bookdown::pdf_document2: default
header-includes: |
  \usepackage{setspace}\onehalfspacing
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{amsmath}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this project we aim to evaluate methods that can account for discrimination
bias in predictive models. Common sources of such bias in datasets are:

* Undersampled groups.
* Having skewed samples.
* Too few variables, i.e. a limitation of features for the model to train on.
* True biases in the world.

How should this be handled (various weighting methods have been proposed)? What
methods are available to be able to evaluate algorithmic fairness? Is there a
difference between modelling methodology when these are applied? The problem of
imbalanced data could be tackled by using synthetic data, i.e. parametric
bootstrapping of the original data set to even out imbalance. But how do
different modelling methods fare when they are being trained on such data?

# Aim

* Identify and apply methods to correct for discrimination bias.
* Evaluate difference between modelling methodology when using synthetic 
  data and when using other methods to handle inherent bias.

# Example dataset

Since our aim is to evaluate methodology rather than to analyse a specific dataset
the choice of data matters most in that we want sets where some bias is present.
Examples include but are not limited to:

* Glassdoor gender pay gap. A dataset containing wage data and demographic
  from Glassdoor, along with education, field, seniority etc. [Source](https://www.kaggle.com/nilimajauhari/glassdoor-analyze-gender-pay-gap).

* Silicon valley diversity data. Diversity of the workforce in Silicon Valley. [Source](https://www.kaggle.com/rtatman/silicon-valley-diversity-data).

* Wages (not ISLR). Containing both gender and race. [Source](https://www.kaggle.com/ljanjughazyan/wages).

* The Demographic /r/ForeverAlone Dataset (very imbalanced between sexes). [Source](https://www.kaggle.com/kingburrito666/the-demographic-rforeveralone-dataset).

# Example analyses



Formulation of the main question that the project aims to answer.
- How can we quantify fairness in order to be able to evaluate algorithmic fairness?
- What methods are available to increase algorithmic fairness? In what type of situations do they apply? (i.e. In what kind of situations can we expect them to be successful?)
Proposition of approaches to the problem and their rationales - based on the lectures and
literature.
Beror på varifrån bias uppstår, vilken typ av variabler vi jobbar med, men vi har föreslagit flera olika angreppssätt.
Some discussion of how the data set should be prepared or modified to meet the requirements
of the methods, if applicable.
N/A lol
Approximate distribution of the efforts and tasks between members of the team.
We plan to work together on both statistical analysis and report writing. We have set up a repository on GitHub in order to collaborate efficiently.











