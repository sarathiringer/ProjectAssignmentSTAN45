---
title: 'Investigating discrimination bias in \newline predictive modelling'
author:
  - Sara Thiringer
  - Jonathan Rittmo
date: "26/10/2020"
output: beamer_presentation
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(summarytools)
library(tidyverse)
library(fairmodels)
library(tidymodels)
library(knitr)
library(kableExtra)
library(DALEXtra)

compas <- fairmodels::compas
COMPAS <- filter(compas, Ethnicity == "African_American" | Ethnicity == "Caucasian") %>% 
  rename(Below25 = Age_Below_TwentyFive) %>% 
  rename(Above45 = Age_Above_FourtyFive)
#compas$Two_yr_Recidivism <- as.factor(ifelse(compas$Two_yr_Recidivism == '1', '0', '1'))
COMPAS$Ethnicity <- droplevels(COMPAS$Ethnicity)


split <- initial_split(COMPAS, prop = 0.8, strata = "Two_yr_Recidivism")
compas_train <- training(split)
y_numeric <- as.numeric(compas_train$Two_yr_Recidivism)-1

```



## Background: The Problem

In recent years, many scandalous examples have shown that statistical models trained on large amounts of data can "act" discriminatory. Examples include:

- Adds of high-income jobs being shown less frequently to women, presumable becasue they've been predicted to be less interested or suitable^[\small @datta_automated_2015\normalsize] 

- Black people's health status being underestimated, leading to inappropriate healtch care measures^[@obermeyer_dissecting_2019]

- Black people begin predicted a higher risk for crime recidivism, leading to higher penalties^[ProPublica (2016)]

## Project Aims

* How can we quantify fairness in order to be able to evaluate algorithmic fairness?
* What methods are available to increase algorithmic fairness? In what type of
  situations do they apply? (i.e. In what kind of situations can we expect them to
  be successful?)

## Background: Why Discrimination Bias?

- Correlation between outcome $y$ and protected charateristic $x_p$
- Correlation between important predictors $\boldsymbol{x}_i$ and protected carachteristic $x_p$
- Under/over sampling of groups with protected carachteristic $x_p$

## Possible Solutions

**Pre-Processing**  | **Training** | **Prediction**
------ | ------|-------------------------------------------
Resampling | Penalty | Threshold
Mapping  | Model bias | adjustments
Altering labels | Tuning for fairness | Alter predictions

We've chosen to work with resampling and threshold adjustment. 

## Possible Goals

Demographic parity

$$
P(Y=1 | X=1) = P(Y=1 | X=0)
$$

Equalized odds

$$
P(G=1|X=0, Y=1) = P(G=1|X=1, Y=1)
$$

<!-- ##  -->

<!-- \small -->
<!-- ```{r desc, results='asis', echo=FALSE} -->

<!-- dfSummary(COMPAS, silent = TRUE, graph.col = FALSE, valid.col = FALSE, na.col = FALSE) -->

<!-- ``` -->
<!-- \normalsize -->

## Data
**Variable**  | **Type** | **Values** | **%** 
------ | ------|-------------------------------------------
Two_yr_Recidivism | Factor | 1 / 0 | 53 / 47
Number_of_Priors  | Numerical | Mean (sd) : 3.5 (4.9) | 36 distinct
Above45 | Factor | 1 / 0 | 79.2 / 20.8
Below25 | Factor | 1 / 0 | 78.1 / 21.9
Misdemeanor | Factor | 1 / 0 | 65.2 / 34.8
Ethnicity | Factor | African_American / Caucasian | 60.2 / 39.8
Sex | Factor | Female / Male | 19.5 / 80.5

## Descriptives

```{r descplots, results='asis', echo=FALSE}

df_des <- filter(compas, Ethnicity == "African_American" | Ethnicity == "Caucasian") %>% 
  rename(Below25 = Age_Below_TwentyFive) %>% 
  rename(Above45 = Age_Above_FourtyFive)

g1 <- ggplot(compas, aes(x = Ethnicity, fill = Ethnicity)) +
  geom_bar() +
  theme_bw() +
  scale_fill_brewer(palette="Dark2") +
  labs(y = "Number of observations") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

g2 <- ggplot(df_des, aes(x = Ethnicity, fill = Ethnicity)) +
  geom_bar() +
  theme_bw() +
  scale_fill_brewer(palette="Dark2") +
  labs(y = "Number of observations")

g3 <- ggplot(df_des, aes(x = Sex, fill = Sex)) +
  geom_bar() +
  theme_bw() +
  scale_fill_brewer(palette="Dark2") +
  labs(y = "Number of observations")

g4 <- ggplot(df_des, aes(x = Two_yr_Recidivism, fill = Two_yr_Recidivism)) +
  geom_bar() +
  theme_bw() +
  scale_fill_brewer(palette="Dark2") +
  labs(y = "Number of observations")

cowplot::plot_grid(g1, g2, g3, g4)

```

## Models

**Model**  | **Tuning** 
------ | -------------------------------------------------
Random Forest | Predictors at each split 
Artificial neural net  | Number of hidden nodes 
Logistic ridge regression | Penalisation 
K-nearest neighbour (left out) | Number of neighbours
AdaBoost | Predictors at each split

## Accuracy and Fairness for the Initial Models

```{r fap1, echo=FALSE, out.width='100%', results='hide', message = FALSE}

load("models/fobject1.Rdata")
fap <- performance_and_fairness(fobject1, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") +
  ylim(0.75, 0.1) +
  xlim(0.635, 0.7) +
      theme(#legend.position = "",
        axis.title=element_text(size=14), aspect.ratio = 0.7)
```

## Disparate Impact Removal

\small
Removes differences between groups while preserving it within the groups. Example: Number of priors. 
\normalsize

```{r dir, results='asis', echo=FALSE, warning=FALSE}


df_rem <- compas_train[,c("Number_of_Priors", "Ethnicity")]
df_rem$Number_of_Priors_Rem1.0 <- disparate_impact_remover(df_rem, protected = 'Ethnicity', features_to_transform = 'Number_of_Priors', lambda = 1.0)$Number_of_Priors

g1 <- ggplot(df_rem, aes(x = Number_of_Priors, color = Ethnicity, fill = Ethnicity)) +
  geom_density(alpha = 0.4) +
  theme_bw() +
  labs(subtitle = "No removal") +
  xlim(c(0, 20)) +
  theme(legend.position = 'none')

g2 <- ggplot(df_rem, aes(x = Number_of_Priors_Rem1.0, color = Ethnicity, fill = Ethnicity)) +
  geom_density(alpha = 0.4) +
  labs(subtitle = "1.0 removal") +
  theme_bw() +
  xlim(c(0, 20)) +
  theme(legend.position = 'none')

cowplot::plot_grid(g1, g2)

```

## Resampling

Using undersampling and oversampling to even out inequalities between the depraved
and privileged groups having positive and negative outcome attributes
respectively. 

```{r joint-org, results='asis', echo=FALSE}

kable(table(COMPAS$Two_yr_Recidivism, COMPAS$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

```

## Uniform Resampling

Aim: Make the joint
distribution of `Ethnicity` and `Two_yr_Recidivism` uniform by duplicating some
observations and removing others.

```{r joint-uni, results='asis', echo=FALSE}
uniform_indexes <- resample(protected = compas_train$Ethnicity,
                            y = y_numeric)

kable(table(compas_train[uniform_indexes,]$Two_yr_Recidivism, compas_train[uniform_indexes,]$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism, uniform resampling",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

```

## Preferential Resampling

- Very similar to uniform 
- Borderline observations skipped or duplicated more often
- Evaluated by logistic regression

```{r joint-pref, results='asis', echo=FALSE}
probs <- glm(Two_yr_Recidivism ~., data = compas_train, family = binomial())$fitted.values
pref_ind <- resample(protected = compas_train$Ethnicity,
                     y = y_numeric,
                     type = "preferential",
                     probs = probs)

kable(table(compas_train[pref_ind,]$Two_yr_Recidivism, compas_train[pref_ind,]$Ethnicity), caption = "Joint distribution of Ethnicity and Recidivism, preferential resampling",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position")) %>% 
  add_footnote(label = 'Note: 1 = Recidivism', notation = "none")

```

## Disparate Impact Removal

```{r fap2, echo=FALSE, out.width='100%', results='hide', message = FALSE}

load("models/fobject2.Rdata")
fap <- performance_and_fairness(fobject2, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") +
  ylim(0.75, 0.1) +
  xlim(0.635, 0.7) +
    theme(legend.position = "",
        axis.title=element_text(size=14), aspect.ratio = 0.7)
```

## Uniform Resampling 

```{r fap3, echo=FALSE, out.width='100%', results='hide', message = FALSE}

load("models/fobject3.Rdata")
fap <- performance_and_fairness(fobject3, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x + 
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") +
  ylim(0.75, 0.1) +
  xlim(0.635, 0.7) +
  theme(legend.position = "",
        axis.title=element_text(size=14), aspect.ratio = 0.7)
```

## Full Comparison

```{r fap-all, echo=FALSE, out.width='100%', results='hide', message=FALSE}

load("models/fobject_all.Rdata")
fap <- performance_and_fairness(fobject_all, fairness_metric = "STP",
                                performance_metric = "accuracy")
x <- plot(fap)
x + 
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") +
  ylim(0.75, 0.1) +
  xlim(0.635, 0.7) +
  theme(legend.position = "",
        axis.title=element_text(size=14), aspect.ratio = 0.7)
```

## Threshold Adjustment 

```{r cutoff-rf, echo=FALSE, out.width='100%',  results='hide', message=FALSE}

load("models/fobject_co.Rdata")

plot(ceteris_paribus_cutoff(fobject_co,
                            subgroup = "African_American",
                            fairness_metrics = c("TPR","STP"))) +
  ggtitle("", subtitle = "") +
  labs(x = "Cutoff value", y = "Parity loss", color = "Parity loss metric") +
  scale_color_discrete(labels = c("Demographic parity", "Equalised odds")) 
  
```

## Comparing Models with Different Thresholds

```{r fap-rf-comp, echo=FALSE, out.width='100%',  results='hide'}

load("models/fo_comp.Rdata")

fap <- performance_and_fairness(fo_comp, fairness_metric = "STP")
plot(fap) +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") 
  
```

## Evaluation on Test Data - Comparison

```{r fap-test, echo=FALSE, out.width='100%', fig.height=6,  results='hide'}

load("models/fobject_test.Rdata")

fap <- performance_and_fairness(fobject_test, fairness_metric = "STP")
plot(fap) +
  ggtitle("") +
  labs(x = "Accuracy", y = "Inversed parity loss (demographic parity)", color = "Model") +
  theme(legend.position = "")
  
```


## Final Model Performance and Conclusion

- The resampling methods seems to have best effect on minimising parity loss while also preserving accuracy rates
- Random Forest looked most promising, but while tested on new data Logistic Regression, Artifical Neural Network and AdaBoost were equally strong competitors (all resampled)

```{r modelperf, results='asis', echo = FALSE}
load("models/rf_test_exp_resa_uni.Rdata")
x <- model_performance(rf_test_exp_resa_uni)
x <- data.frame(x[["measures"]]) %>% 
  select(-f1) %>% round(digits = 3)
kable(x, caption = "Performance",
      format = "latex", booktabs = T)%>%
  kable_styling(latex_options = c("hold_position"))

```


## General Conclusions

- It is possible to build models who satisfy some fairness criteria without a too large drop in accuracy
- Decisions for fair models include: fairness measure, evaluation metrics and choice of methods
- Less complex methods can be found amongst resampling and threshold adjustment
- As always, what type of data we are dealing with will largely impact the results of different methods (for example why disparate impact remover didn't work)

## References

