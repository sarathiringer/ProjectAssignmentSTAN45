---
title: 'Project Proposal'
author:
  - Jonathan Rittmo
  - Sara Thiringer
date: "`r Sys.Date()`"
# bibliography: Dissertation.bib
toc: false
link-citations: yes
linkcolor: blue
numbersections: false
indent: true
csl: apa.csl
output: 
  bookdown::pdf_document2: default
header-includes: |
  \usepackage{setspace}\onehalfspacing
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{amsmath}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this project we aim to evaluate methods that can account for discrimination
bias in predictive models. This can for example happen when a dataset on which
the model is based is unbalanced with regards to a certain group, i.e.
observations are fewer in one group compared to another. Bias can also be
introduced in a model because the data reflect true discrimination bias in the
population. Take an employee wage data set as an example. If a model, e.g.,
predicts equal wages between two groups when all variables are available except
for this specific grouping (say gender) have been taking into account but
different wages when gender is introduced there is evidence of true bias.

Common sources of similar bias in datasets are:

* Undersampled groups.
* Having skewed samples.
* Too few variables, i.e. a limitation of features for the model to train on.
* True biases in the world.

How should this be handled? The problem of imbalanced data could be tackled by
using synthetic data, i.e. parametric bootstrapping of the original data set 
to even out imbalance. But how do different modelling methods fare when they
are being trained on such data?

The second problem is more difficult. In the example given above the easy way
to deal with such a problem is just to remove the biased variable for model 
training. However, when the problematic variable is correlated with
other predictors in the model the bias can persist even after removal.

One way to handle this is to build algorithms optimising for fairness.
This is most often done by operationalising a non-discrimination criterion.

## Demographic parity

This criterion states that the target variable should be independent of
problematic, let us call them "protected", variables (such as race and gender).
In other words this means that if $Y$ is our decision and $X$ the problematic
variable in question:
$$
P(Y=1 | X=1) = P(Y=1 | X=0)
$$
However, this makes sure we cannot use $X$ as a predictor which we in some
circumstances want.

## Equalised odds

This criterion proposes that the predictor we use and the
protected variable should be independent conditional on the outcome, i.e.
if $G$ is the predictor we want to use:
$$
P(G=1|X=0, Y=1) = P(G=1|X=1, Y=1)
$$
## Well-calibrated systems

Similar to equalised odds, but this criterion propose that the *target*
and the prtected variable are independent, conditional on the *predictor*.
$$
P(Y=1|X=0, G=1) = P(Y=1|X=1, G=1)
$$

Applying any of the above criteria will require careful consideration when
choosing variables to be included in a given model and some accuracy must perhaps
be sacrificed. We therefore wish to evaluate how well these criteria would fare
when applied to different kinds of models and datasets. 

# Aim

* Evaluate non-discrimination criteria and how the fare under various models and datasets.
* Evaluate the performance of the used models when trained on synthetic data.
* Identify a method for quantifying fairness and compare accuracy vs. fairness.

# Example dataset

Since our aim is to evaluate methodology rather than to analyse a specific dataset
the choice of data matters most in that we want sets where some bias is present.
Examples include but are not limited to:

* Glassdoor gender pay gap. A dataset containing wage data and demographic
  from Glassdoor, along with education, field, seniority etc. [Source](https://www.kaggle.com/nilimajauhari/glassdoor-analyze-gender-pay-gap).

* Silicon valley diversity data. Diversity of the workforce in Silicon Valley. [Source](https://www.kaggle.com/rtatman/silicon-valley-diversity-data).

* Wages (not ISLR). Containing both gender and race. [Source](https://www.kaggle.com/ljanjughazyan/wages).

* The Demographic /r/ForeverAlone Dataset (very imbalanced between sexes). [Source](https://www.kaggle.com/kingburrito666/the-demographic-rforeveralone-dataset).

# Example analyses

Since applying the non-discrimiation criteria will require extensive work
for each dataset what is given here is solely an example of how we might 
tackle this issue for the /r/ForeverAlone dataset. This dataset consists
of the variables of interest (I'm just listing all of them now but perhaps we should remove a few?):

* `gender`
* `sexuality`
* `age`
* `income`
* `race`
* `bodyweight`
* `virgin`
* `prostitution_legal`
* `pay_for_sex`
* `friends`
* `social_fear`
* `depressed`
* `what_help_from_others`
* `attempt_suicide`
* `employment`
* `job_title`
* `edu_level`
* `improve_yourself_how`

In this analysis we want a model able to predict the risk of an individual
attempting to commit suicide where `gender` is our protected variable. First 
and foremost we would need to create synthetic data to even out the imbalance
between gender groups. For simplicity, observations with other genders than male
or female will be discarded in this example. This would be done by parametric
bootstrapping of variables where distributions are estimated by the female sample 
in the dataset. Since most variables are nominal with only a few levels most can
be estimated with a binomial distribution.

We would then need to look at the joint distributions of the variables of interest












